{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "try:\n",
    "    __file__\n",
    "except:\n",
    "    sys.argv = [sys.argv[0],'--dynet-gpu', '--dynet-mem', '4000']\n",
    "import matplotlib.pyplot as plt\n",
    "import dynet as dy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14\n",
      "14\n",
      "14\n",
      "14\n",
      "14\n",
      "14\n",
      "14\n",
      "14\n",
      "14\n",
      "14\n",
      "14\n",
      "14\n",
      "14\n",
      "14\n",
      "14\n",
      "12\n",
      "14\n",
      "15\n",
      "15\n",
      "14\n",
      "14\n",
      "16\n",
      "15\n",
      "15\n",
      "14\n",
      "14\n",
      "14\n",
      "15\n",
      "15\n",
      "13\n",
      "15\n",
      "14\n",
      "15\n",
      "14\n",
      "15\n",
      "15\n",
      "15\n"
     ]
    }
   ],
   "source": [
    "def parse_level(file_name):\n",
    "    with open(file_name) as input_file:\n",
    "        rows = [list(line.rstrip()) for line in input_file]\n",
    "        column_len = len(rows[0])\n",
    "        print len(rows)\n",
    "        data = ['*start*','(']\n",
    "        flip = True\n",
    "        for c in range(column_len):\n",
    "            if flip:\n",
    "                for r in range(len(rows)):\n",
    "                    data.append(rows[r][c])\n",
    "                data.append(')')\n",
    "            else:\n",
    "                for r in range(len(rows)-1,-1,-1):\n",
    "                    data.append(rows[r][c])\n",
    "                data.append('(')\n",
    "            flip = not flip\n",
    "            if c % 10 == 0:\n",
    "                data += ['^']*(int(c/10))\n",
    "        data.append('*end*')\n",
    "    return data\n",
    "\n",
    "\n",
    "levels = ['TheVGLC-master/Super Mario Bros/Processed/mario-1-1.txt',\n",
    "    'TheVGLC-master/Super Mario Bros/Processed/mario-1-2.txt',\n",
    "    'TheVGLC-master/Super Mario Bros/Processed/mario-1-3.txt',\n",
    "    'TheVGLC-master/Super Mario Bros/Processed/mario-2-1.txt',\n",
    "    'TheVGLC-master/Super Mario Bros/Processed/mario-3-1.txt',\n",
    "    'TheVGLC-master/Super Mario Bros/Processed/mario-3-3.txt',\n",
    "    'TheVGLC-master/Super Mario Bros/Processed/mario-4-1.txt',\n",
    "    'TheVGLC-master/Super Mario Bros/Processed/mario-4-2.txt',\n",
    "    'TheVGLC-master/Super Mario Bros/Processed/mario-5-1.txt',\n",
    "    'TheVGLC-master/Super Mario Bros/Processed/mario-5-3.txt',\n",
    "    'TheVGLC-master/Super Mario Bros/Processed/mario-6-1.txt',\n",
    "    'TheVGLC-master/Super Mario Bros/Processed/mario-6-2.txt',\n",
    "    'TheVGLC-master/Super Mario Bros/Processed/mario-6-3.txt',\n",
    "    'TheVGLC-master/Super Mario Bros/Processed/mario-7-1.txt',\n",
    "    'TheVGLC-master/Super Mario Bros/Processed/mario-8-1.txt',\n",
    "    'TheVGLC-master/Super Mario Bros 2 (Japan)/Processed/SuperMarioBros2(J)-World1-1.txt',\n",
    "    'TheVGLC-master/Super Mario Bros 2 (Japan)/Processed/SuperMarioBros2(J)-World1-2.txt',\n",
    "    'TheVGLC-master/Super Mario Bros 2 (Japan)/Processed/SuperMarioBros2(J)-World1-3.txt',\n",
    "    'TheVGLC-master/Super Mario Bros 2 (Japan)/Processed/SuperMarioBros2(J)-World2-1.txt',\n",
    "    'TheVGLC-master/Super Mario Bros 2 (Japan)/Processed/SuperMarioBros2(J)-World2-2.txt',\n",
    "    'TheVGLC-master/Super Mario Bros 2 (Japan)/Processed/SuperMarioBros2(J)-World2-3.txt',\n",
    "    'TheVGLC-master/Super Mario Bros 2 (Japan)/Processed/SuperMarioBros2(J)-World3-1.txt',\n",
    "    'TheVGLC-master/Super Mario Bros 2 (Japan)/Processed/SuperMarioBros2(J)-World3-3.txt',\n",
    "    'TheVGLC-master/Super Mario Bros 2 (Japan)/Processed/SuperMarioBros2(J)-World4-1.txt',\n",
    "    'TheVGLC-master/Super Mario Bros 2 (Japan)/Processed/SuperMarioBros2(J)-World4-2.txt',\n",
    "    'TheVGLC-master/Super Mario Bros 2 (Japan)/Processed/SuperMarioBros2(J)-World4-3.txt',\n",
    "    'TheVGLC-master/Super Mario Bros 2 (Japan)/Processed/SuperMarioBros2(J)-World5-1.txt',\n",
    "    'TheVGLC-master/Super Mario Bros 2 (Japan)/Processed/SuperMarioBros2(J)-World5-2.txt',\n",
    "    'TheVGLC-master/Super Mario Bros 2 (Japan)/Processed/SuperMarioBros2(J)-World6-1.txt',\n",
    "    'TheVGLC-master/Super Mario Bros 2 (Japan)/Processed/SuperMarioBros2(J)-World6-3.txt',\n",
    "    'TheVGLC-master/Super Mario Bros 2 (Japan)/Processed/SuperMarioBros2(J)-World8-1.txt',\n",
    "    'TheVGLC-master/Super Mario Bros 2 (Japan)/Processed/SuperMarioBros2(J)-WorldA-1.txt',\n",
    "    'TheVGLC-master/Super Mario Bros 2 (Japan)/Processed/SuperMarioBros2(J)-WorldA-3.txt',\n",
    "    'TheVGLC-master/Super Mario Bros 2 (Japan)/Processed/SuperMarioBros2(J)-WorldB-1.txt',\n",
    "    'TheVGLC-master/Super Mario Bros 2 (Japan)/Processed/SuperMarioBros2(J)-WorldB-3.txt',\n",
    "    'TheVGLC-master/Super Mario Bros 2 (Japan)/Processed/SuperMarioBros2(J)-WorldC-2.txt',\n",
    "    'TheVGLC-master/Super Mario Bros 2 (Japan)/Processed/SuperMarioBros2(J)-WorldD-1.txt',    \n",
    "]\n",
    "\n",
    "\n",
    "levels = [parse_level(l) for l in levels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "v = [set(level) for level in levels]\n",
    "v = reduce(lambda x,y: x | y,v)\n",
    "\n",
    "v2i = {v:i for i,v in enumerate(sorted(v))}\n",
    "i2v = {i:v for v,i in v2i.items()}\n",
    "\n",
    "embeddings_size = len(v2i)\n",
    "encoder_state_size = 512\n",
    "\n",
    "encoding_size = 64\n",
    "\n",
    "dropout_max = 0.8\n",
    "dropout_min = 0.0\n",
    "dropout_midpoint = -100\n",
    "dropout_steepness = 0.125\n",
    "\n",
    "decoder_state_size = 512\n",
    "layers = 2\n",
    "total = 0\n",
    "\n",
    "model = dy.Model()\n",
    "\n",
    "encoder_embedding = model.add_lookup_parameters((len(v2i), embeddings_size))\n",
    "encoder_forward_rnn = dy.LSTMBuilder(layers, embeddings_size, encoder_state_size, model)\n",
    "encoder_backward_rnn = dy.LSTMBuilder(layers, embeddings_size, encoder_state_size, model)\n",
    "#encoder_backward_rnn.set_dropout(dropout_min)\n",
    "\n",
    "encoder_w_mean = model.add_parameters((encoding_size, encoder_state_size*2)) \n",
    "encoder_b_mean = model.add_parameters((encoding_size))\n",
    "\n",
    "encoder_w_logvar = model.add_parameters((encoding_size, encoder_state_size*2)) \n",
    "encoder_b_logvar = model.add_parameters((encoding_size))\n",
    "\n",
    "decoder_rnn = dy.LSTMBuilder(layers, encoding_size+embeddings_size, decoder_state_size, model)\n",
    "decoder_rnn.set_dropout(dropout_min)\n",
    "\n",
    "\n",
    "decoder_w = model.add_parameters((len(v2i), decoder_state_size)) \n",
    "decoder_b = model.add_parameters((len(v2i)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "    \n",
    "trainer = dy.RMSPropTrainer(model)\n",
    "\n",
    "def run_rnn(init_state, input_vecs):\n",
    "    s = init_state\n",
    "\n",
    "    states = s.add_inputs(input_vecs)\n",
    "    rnn_outputs = [s.output() for s in states]\n",
    "    return rnn_outputs\n",
    "    \n",
    "def encode_string(embedded_string,rnn):\n",
    "    initial_state = rnn.initial_state()\n",
    "\n",
    "    # run_rnn returns all the hidden state of all the slices of the RNN\n",
    "    hidden_states = run_rnn(initial_state, embedded_string)\n",
    "\n",
    "    return hidden_states\n",
    "\n",
    "def embed_string(embeddings, string):\n",
    "    return [embeddings[tok] for tok in string]\n",
    "\n",
    "def get_loss(level,loss_annealing=1.0):  \n",
    "    dy.renew_cg()      \n",
    "    states = []\n",
    "    \n",
    "    embedded_level = embed_string(encoder_embedding,level)\n",
    "    \n",
    "    forward_encoding = encode_string(embedded_level,encoder_forward_rnn)\n",
    "    backward_encoding = encode_string(embedded_level[::-1],encoder_backward_rnn)\n",
    "    \n",
    "    encoder_w_mean_ = dy.parameter(encoder_w_mean)\n",
    "    encoder_b_mean_ = dy.parameter(encoder_b_mean)\n",
    "    \n",
    "    encoder_w_logvar_ = dy.parameter(encoder_w_logvar)\n",
    "    encoder_b_logvar_ = dy.parameter(encoder_b_logvar)\n",
    "    \n",
    "    encoder_mean = encoder_w_mean_*dy.concatenate([forward_encoding[-1],backward_encoding[-1]])+encoder_b_mean_\n",
    "    encoder_logvar = encoder_w_logvar_*dy.concatenate([forward_encoding[-1],backward_encoding[-1]])+encoder_b_logvar_\n",
    "    \n",
    "    epsilon = dy.noise(dy.vecInput(encoding_size),1.0)\n",
    "    encoded_state = dy.tanh(encoder_mean + dy.cmult(dy.exp(encoder_logvar / 2.0), epsilon))\n",
    "     \n",
    "    decoder_state = decoder_rnn.initial_state()\n",
    "    decoder_state = decoder_state.add_input(dy.concatenate([encoded_state,encoder_embedding[level[0]]]))\n",
    "    \n",
    "    output_w = dy.parameter(decoder_w)\n",
    "    output_b = dy.parameter(decoder_b)\n",
    "    loss = []\n",
    "    for tile in level[1:]:\n",
    "        rnn_output = decoder_state.output()\n",
    "        probs = dy.softmax(output_w * rnn_output + output_b)\n",
    "        loss.append(-dy.log(dy.pick(probs, tile)))\n",
    "        decoder_state = decoder_state.add_input(dy.concatenate([encoded_state,encoder_embedding[tile]]))\n",
    "    decoder_state = decoder_state.add_input(dy.concatenate([encoded_state,encoder_embedding[tile]]))\n",
    "    \n",
    "    kl_loss = - 0.5 *dy.sum_elems(1 + encoder_logvar - dy.square(encoder_mean) - dy.exp(encoder_logvar))\n",
    "    \n",
    "    print kl_loss.value()\n",
    "    \n",
    "    loss = dy.esum(loss)+kl_loss*loss_annealing\n",
    "    return loss\n",
    "    \n",
    "def train(level,loss_annealing=1.0):  \n",
    "    loss = get_loss(level,loss_annealing)\n",
    "    loss.backward()\n",
    "    trainer.update()\n",
    "    return loss\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 15 0.00385346932665\n",
      "TRAINING\n",
      "10.9191036224\n",
      "10.5480699539\n",
      "10.3432416916\n",
      "10.1097297668\n",
      "10.3153553009\n",
      "10.7473583221\n",
      "10.7198705673\n",
      "11.5105371475\n",
      "12.9115877151\n",
      "13.5326557159\n",
      "15.022860527\n",
      "15.785235405\n",
      "16.5165481567\n",
      "18.1645641327\n",
      "17.6555023193\n",
      "18.1155662537\n",
      "18.4831943512\n",
      "20.258441925\n",
      "22.4704647064\n",
      "23.7031421661\n",
      "23.1587638855\n",
      "23.4784221649\n",
      "22.965511322\n",
      "21.0526981354\n",
      "23.7891082764\n",
      "25.1963748932\n",
      "25.643163681\n",
      "24.2089614868\n",
      "24.455165863\n",
      "1.03072027619\n",
      "VALIDATION\n",
      "25.0137233734\n",
      "25.0126781464\n",
      "25.0129184723\n",
      "25.0123729706\n",
      "25.0137195587\n",
      "25.0134048462\n",
      "25.0127658844\n",
      "25.013710022\n",
      "1.06323936805\n",
      "epoch 16 0.00436543289069\n",
      "TRAINING\n",
      "25.0126781464\n",
      "25.090883255\n",
      "24.3426437378\n",
      "24.4013500214\n",
      "23.1211242676\n",
      "24.0420055389\n",
      "25.4293727875\n",
      "26.9511451721\n",
      "26.9395923615\n",
      "27.1704483032\n",
      "27.7935218811\n",
      "27.0923595428\n",
      "27.145029068\n",
      "25.9347648621\n",
      "26.3763885498\n",
      "26.2804679871\n",
      "27.8457298279\n",
      "28.5434494019\n",
      "29.2114448547\n",
      "29.9010334015\n",
      "29.7951412201\n",
      "28.9142608643\n",
      "28.8429832458\n",
      "29.840139389\n",
      "29.7727375031\n",
      "29.7037143707\n",
      "31.1793327332\n",
      "32.360168457\n",
      "34.0661506653\n",
      "1.02751278536\n",
      "VALIDATION\n",
      "34.1306152344\n",
      "34.1296195984\n",
      "34.1298675537\n",
      "34.1296424866\n",
      "34.130607605\n",
      "34.1305084229\n",
      "34.1297988892\n",
      "34.1305961609\n",
      "1.0399590871\n",
      "epoch 17 0.00494524631327\n",
      "TRAINING\n",
      "34.1296463013\n",
      "33.1663284302\n",
      "32.987121582\n",
      "31.9430675507\n",
      "31.6686477661\n",
      "31.2840118408\n",
      "32.3679618835\n",
      "32.2168273926\n",
      "33.4598655701\n",
      "31.5022449493\n",
      "30.4664096832\n",
      "30.1739597321\n",
      "32.1321716309\n",
      "30.6106300354\n",
      "29.4606361389\n",
      "28.7904472351\n",
      "30.8122272491\n",
      "29.8812103271\n",
      "32.1534996033\n",
      "32.1806716919\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "\n",
    "random.shuffle(levels)\n",
    "training_set = levels[:int(len(levels)*0.8)]\n",
    "validation_set = levels[int(len(levels)*0.8):]\n",
    "best = float('inf')\n",
    "\n",
    "\n",
    "\n",
    "kl_max = 2.0\n",
    "kl_min = 0.0\n",
    "kl_midpoint = 50\n",
    "kl_steepness = 0.125\n",
    "\n",
    "\n",
    "for ii in range(2000):\n",
    "    total += 1\n",
    "    dropout = dropout_min + dropout_max/(1.0+np.exp(-dropout_steepness*(ii-dropout_midpoint)))\n",
    "    kl = kl_min + kl_max/(1.0+np.exp(-kl_steepness*(ii-kl_midpoint)))\n",
    "    \n",
    "    print 'epoch', total,kl\n",
    "    #encoder_forward_rnn.set_dropout(dropout)\n",
    "    decoder_rnn.set_dropout(dropout)\n",
    "\n",
    "    random.shuffle(training_set)\n",
    "    print 'TRAINING'\n",
    "    total_train = 0\n",
    "    total_size = 0\n",
    "    for level in training_set:\n",
    "        total_size += len(level)\n",
    "        total_train += train([v2i[tile] for tile in level],kl).value()\n",
    "    print total_train/float(total_size)\n",
    "    print 'VALIDATION'\n",
    "    total_validation = 0\n",
    "    total_size = 0\n",
    "    \n",
    "    for level in validation_set:\n",
    "        val_loss =  get_loss([v2i[tile] for tile in level],kl).value()\n",
    "        total_size += len(level)\n",
    "        total_validation += val_loss\n",
    "    total_validation = total_validation/float(total_size)\n",
    "    print total_validation\n",
    "    if total_validation < best:\n",
    "        best = total_validation\n",
    "        model.save('{}_{}_{}_{}_{}_{}.model'.format(encoder_state_size,decoder_state_size,layers,dropout_max,encoding_size,total_validation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import numpy.random\n",
    "\n",
    "def encode(level):\n",
    "    dy.renew_cg() \n",
    "    embedded_level = embed_string(encoder_embedding,level)\n",
    "    forward_encoding = encode_string(embedded_level,encoder_forward_rnn)\n",
    "    backward_encoding = encode_string(embedded_level[::-1],encoder_backward_rnn)\n",
    "    encoder_w_ = dy.parameter(encoder_w)\n",
    "    encoder_b_ = dy.parameter(encoder_b)\n",
    "    encoded_state = dy.tanh(encoder_w_*dy.concatenate([forward_encoding[-1],backward_encoding[-1]])+encoder_b_) \n",
    "   \n",
    "    return encoded_state.value()\n",
    "\n",
    "\n",
    "def sample(preds, temperature=1.0):\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)\n",
    "\n",
    "\n",
    "def generate(encoding,max_length=7000,temperature=1.0):\n",
    "    dy.renew_cg() \n",
    "    encoding = dy.inputTensor(encoding)\n",
    "    decoder_state = decoder_rnn.initial_state()\n",
    "    decoder_state = decoder_state.add_input(dy.concatenate([encoding,\n",
    "                                               encoder_embedding[v2i['*start*']]]))\n",
    "    \n",
    "    decoded = []\n",
    "    \n",
    "    output_w = dy.parameter(decoder_w)\n",
    "    output_b = dy.parameter(decoder_b)\n",
    "    for ii in xrange(max_length):\n",
    "        rnn_output = decoder_state.output()\n",
    "        probs = dy.softmax(output_w * rnn_output + output_b)\n",
    "        sampled = sample(probs.value(),temperature)\n",
    "        decoded.append(sampled)\n",
    "        decoder_state = decoder_state.add_input(dy.concatenate([encoding,\n",
    "                                                                encoder_embedding[sampled]]))\n",
    "        if i2v[sampled] == '*end*':\n",
    "            break\n",
    "            \n",
    "    return decoded\n",
    "\n",
    "level = levels[1]\n",
    "\n",
    "\n",
    "\n",
    "encoded = encode([v2i[tile] for tile in level])    \n",
    "temp = 1.0\n",
    "\n",
    "for ii in range(50):\n",
    "    generated = generate(encoded,7000,temp)\n",
    "\n",
    "    generated_str = [i2v[tok] for tok in generated]\n",
    "    direction = -1\n",
    "    current = 0\n",
    "    level = []\n",
    "    col = None\n",
    "    for tok in generated_str:\n",
    "        if tok == '(':\n",
    "            if col:\n",
    "                level.append(col)\n",
    "            col = ['-']*14\n",
    "            direction = 1\n",
    "            current = 0\n",
    "        elif tok == ')':\n",
    "            if col:\n",
    "                level.append(col)\n",
    "            col = ['-']*14\n",
    "            direction = -1\n",
    "            current = 13\n",
    "        elif tok == '^':\n",
    "            pass\n",
    "        elif tok == '*end*':\n",
    "            pass\n",
    "        else:\n",
    "            if current < 0:\n",
    "                current = 0\n",
    "            if current > 13:\n",
    "                current = 13\n",
    "            col[current] = tok\n",
    "\n",
    "            current += direction\n",
    "\n",
    "    level_oriented = [list('-'*len(level)) for ii in range(len(col))]\n",
    "    for ii in range(len(level)):\n",
    "        for jj in range(len(level[ii])):\n",
    "\n",
    "            level_oriented[jj][ii] = level[ii][jj]\n",
    "\n",
    "    for row in level_oriented:\n",
    "        print ''.join(row)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.hist(np.array(encoded),250)\n",
    "plt.show()\n",
    "\n",
    "for level in levels:\n",
    "    print len(level)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
